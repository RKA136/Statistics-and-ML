{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aaa20af",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c79a4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Different algorithms work best on different data types.\n",
    "- In 70c-80s\n",
    "    - linear regression → works best on linear data\n",
    "    - Naive Base → works best on textual data\n",
    "- In 90s - more general algorithms are created\n",
    "    - Random Forest\n",
    "    - Support Vector Mechine\n",
    "    - Gradient Boosting\n",
    "        - All of them are quite fast and mostly work on all type of data\n",
    "        - But overfitting issues and scalability (increase in dataset size was not handeled properly → slow)\n",
    "- In 2014 → Came XGBoost\n",
    "    - Not an algorithm, but a library, created on already existing gradient boosting algorithm.\n",
    "    - **Tinaqi Chen** → potential in Gradient boosting.\n",
    "    - XGBoots is ML from Gradient boosting and lots of software engenearing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87246fad",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "### Early days (2014)\n",
    "- Why Gradient Boosting is the candidate?\n",
    "    - Flexibility (any loss function can be used → enables to work on different problems e.g. regression, classification, ranking, custom defined problems)\n",
    "    - Performance\n",
    "    - Robust → Regularization and missing values\n",
    "    - Kaggle\n",
    "- Improvement needed on performance and big data handling\n",
    "- Tianqi Chen created XGBoost\n",
    "\n",
    "### Kaggle pe Bawaal (2016)\n",
    "- Tianqi Chen started to participate in Kaggle to test the performance of the XGBoost.\n",
    "- He himself participated in **Higgs Boson Competition** → Identification of particles by using ML.\n",
    "- More people started using XGBoost.\n",
    "- Better results → Increase in popularity\n",
    "\n",
    "### Open Source (2016 onwards)\n",
    "- Tianqi made the XGBoost open source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a5a6f",
   "metadata": {},
   "source": [
    "## Links for XGBoost\n",
    "[XGBoost](https://xgboost.ai/)\n",
    "\n",
    "[XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f46e0f",
   "metadata": {},
   "source": [
    "## XGBoost Features\n",
    "\n",
    "- Major area of concerns \n",
    "    - Performance\n",
    "    - Speed \n",
    "    - Flexibility\n",
    "\n",
    "1. Flexibility\n",
    "    1) Cross Platform\n",
    "    2) Multiple Language Support\n",
    "    3) Integration with other libraries and tools\n",
    "    4) Support all kinds of ML problems\n",
    "\n",
    "2. Speed\n",
    "    1) Parallel Processing\n",
    "    2) Optimized Data Structure\n",
    "    3) Cache Awareness\n",
    "    4) Out of Core Computing\n",
    "    5) Distributed Computing\n",
    "    6) GPU Support\n",
    "\n",
    "3. Performance\n",
    "    1) Regularized Learning Objectives\n",
    "    2) Handling Missing Values\n",
    "    3) Sparcity Aware Split Finding\n",
    "    4) Efficient Split Finding (Weighted Quantile Sketch + Approximate Tree Learning)\n",
    "    5) Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bbc07",
   "metadata": {},
   "source": [
    "## Introduction to Boosted Trees\n",
    "\n",
    "### Model and Parameters\n",
    "- The model is basically the mathematical structure by which the prediction $y_i$ is made from the inputs $x_i$.\n",
    "- Linear Model\n",
    "    $y_i = \\sum_j θ_j×x_{ij}$ → The linear combination of the weighted input features.\n",
    "- The parameter is the undetermined part that we need to learn from the data.\n",
    "- In linear regression problems the parameters are the coefficients θᵢ.\n",
    "\n",
    "### Objective Function: Training Loss + Regularization \n",
    "- The task of training the model is basically finding the best parameters θ that best fit the training data xᵢ and the predictions yᵢ.\n",
    "- In order to train the model, we need to debhfine the **objective function** to measure how well the model fit the training data.\n",
    "\n",
    "- **Objective function**\n",
    "- Training Loss + Regularization term\n",
    "- obj(θ) = L(θ) + Ω(θ)\n",
    "    - L → Training Loss \n",
    "    - Ω → Regularization term\n",
    "\n",
    "- A common Choice of L(θ) is the mean squared error\n",
    "    - $L(θ) = \\sum_i(y_i-\\hat{y}_i)^2$\n",
    "- Another one is logistic loss\n",
    "    - $L(θ) = \\sum_i(y_iln(1+e^{-\\hat{y}_i})+(1-y_i)ln(1+e^{\\hat{y}_i}))$\n",
    "\n",
    "- The regularization term controls the complexity of the model, which helps to avoid overfitting.\n",
    "\n",
    "- A visual representation\n",
    "\n",
    "<img src=\"XGBoost_1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee48e9d",
   "metadata": {},
   "source": [
    "## Decision Tree Ensembles\n",
    "\n",
    "- The tree ensemble model consists of a set of classification and regression trees (CART).\n",
    "- Mathematically we can write our model in the form\n",
    "\n",
    "$\\hat{y}_i =  \\sum_{k =1 }^Kf_k(x_i), \\quad   f_k ϵ \\mathcal{F}$\n",
    "\n",
    "- K is the number of trees \n",
    "- f_k is a function in the functional space $\\mathcal{F}$\n",
    "- $\\mathcal{F}$ is the set of all posible CARTs.\n",
    "\n",
    "- The objective function to be optimised is \n",
    "\n",
    "$obj(\\theta) = \\sum_i^nl(y_i,\\hat{y}_i^)+\\sum_{k=1}^K \\omega(f_k)$\n",
    "\n",
    "- where $\\omega(f_k)$ is the complexity of the tree $f_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78ed86",
   "metadata": {},
   "source": [
    "## Tree Boosting\n",
    "- How should we learn the trees?\n",
    "    - *Define an objective function and optimize it*.\n",
    "\n",
    "- Let the objective function be \n",
    "\n",
    "    $obj = \\sum_{i=1}^n l(y_i,\\hat{y_i}^{(t)}) + \\sum_{k=1}^t \\omega(f_k)$\n",
    "\n",
    "- In which $t$ is the number of trees in our ensemble. (Each step will add one new tree, so that at step t the ensemble contains $K=t$ trees).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a963dbb",
   "metadata": {},
   "source": [
    "### Additive Training\n",
    "- Fix what we have learned and add one new tree at a time. \n",
    "- Write prediction value at step $t$ as $\\hat{y}_i^{(t)}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}_i^{(0)} &= 0\\\\\n",
    "\\hat{y}_i^{(1)} &= f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)\\\\\n",
    "\\hat{y}_i^{(2)} &= f_1(x_i) +f_2(x_i) = \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "\\ldots&\\\\\n",
    "\\hat{y}_i^{(t)} &= \\sum_{k =1 }^t f_k(x_i) = \\hat{y}_i^{(t-1)} +f_t(x_i)\n",
    "\\end{align}\n",
    "\n",
    "- Which tree to add at each step?\n",
    "    - Add the one that optimized our objective\n",
    "    \n",
    "    \\begin{align}\n",
    "    obj^{(t)} &= \\sum_{i=1}^n l(y_i,\\hat{y_i}^{(t)})+\\sum_{k=1}^K \\omega(f_k)\\\\\n",
    "                &= \\sum_{i=1}^n l(y_i,\\hat{y_i}^{(t-1)}+f_t(x_i)) + \\omega(f_t) + \\text{constant}\n",
    "    \\end{align}\n",
    "\n",
    "    - If we use mean square error as our loss function\n",
    "\n",
    "        \\begin{align} \n",
    "        obj^{(t)} &= \\sum_{i=1}^n(y_i - (\\hat{y_i}^{(t-1)}+f_t(x_i)))^2 + \\sum_{k=1}^t\\omega(f_k)\\\\\n",
    "                  &= \\sum_{i=1}^n[2(\\hat{y}_i^{(t-1)}-y_i)f_t(x_i)+f_t(x_i)^2] + \\omega(f_t) + \\text{constant}\n",
    "        \\end{align}\n",
    "    - If we use other losses of interest(Logistic loss) the calculation is not so simple and we take the Taylor expansion of the loss function upto second order\n",
    "    \n",
    "        $obj^{(t)} = \\sum_{i=1}^n[l(y_i,\\hat{y}_i^{(t-1)}) + g_if_t(x_i) + \\frac{1}{2} h_if_t^2(x_i)] + \\omega(f_t) + \\text{constant}$\n",
    "\n",
    "        - $g_i = \\partial_{\\hat{y}_i^{(t-1)}}l(y_i,\\hat{y}_i^{(t-1)})$\n",
    "        - $h_i = \\partial_{\\hat{y}_i^{(t-1)}}^2l(y_i,\\hat{y}_i^{(t-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29def7d2",
   "metadata": {},
   "source": [
    "### Model Complexity\n",
    "- The regularization term.\n",
    "- We need to define the complexity of the tree $\\omega(f)$.\n",
    "- Refine the definition of the tree $f(x)$ as \n",
    "\n",
    "    $f_t(x) = w_{q(x)}, \\quad w ∈ R^T, \\quad q:R^d → {1,2,\\ldots,T}$.\n",
    "\n",
    "    - Here $w$ is the vector of scores on leaves \n",
    "    - $q$ is a function assigning each data point to the corresponding leaf\n",
    "    - $T$ is the number of leaves.\n",
    "\n",
    "- In XGBoost we define the complexity as \n",
    "\n",
    "\\begin{align*}\n",
    "\\omega(f) = γT + \\frac{1}{2}\\lambda\\sum_{j=1}^T w_j^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68586600",
   "metadata": {},
   "source": [
    "### The Structure Score\n",
    "- After reformulating the tree model, we can write the objective value at the $t$-th tree as:\n",
    "\n",
    "\\begin{align*}\n",
    "obj^{(t)} &≈ \\sum_{i=1}^n[g_iw_{q(x_i)}+\\frac{1}{2}h_iw_{q(x_i)}^2] + γT + \\frac{1}{2}\\lambda\\sum_{j=1}^Tw_j^2\\\\\n",
    "          &= \\sum_{j=1}^T[\\left(\\sum_{i∈ I_j}g_i\\right)w_j + \\frac{1}{2}\\left(\\sum_{i∈I_j}h_i+\\lambda\\right)w_j^2] + γT\n",
    "\\end{align*}\n",
    "- where $I_j = \\{i|q(x_i)=j\\}$  is the set of indices of data points assigned to the $j$-th leaf.\n",
    "\n",
    "- Defining $G_j = \\sum_{i∈ I_j}g_i$ and $H_j = \\sum_{i∈ I_j}h_i$\n",
    "\n",
    "\\begin{align*}\n",
    "    obj^{(t)} =  \\sum_{j=1}^T[G_jw_j + \\frac{1}{2}\\left(H_j+\\lambda\\right)w_j^2] + γT\n",
    "\\end{align*}\n",
    "- In this equation $w_j$ are independent with respect to each other.\n",
    "- The form $[G_jw_j + \\frac{1}{2}\\left(H_j+\\lambda\\right)w_j^2$ is quadratic and the best $w_j$ for a given structure $q(x)$ and the best objective reduction we can get is:\n",
    "\n",
    "    \\begin{align*}\n",
    "    w_j^* &= - \\frac{G_j}{H_j + \\lambda}\\\\\n",
    "    obj^* &= - \\frac{1}{2}\\sum_{j=1}^T \\frac{G_j^2}{H_j + \\lambda} + γT\n",
    "    \\end{align*}\n",
    "\n",
    "- The last equation measures how good a tree structure $q(x)$ is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa480008",
   "metadata": {},
   "source": [
    "### Learn The Tree Structure\n",
    "- We try to split a leaf into two leaves, and the score it gains is \n",
    "\n",
    "$Gain = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_l +G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma$\n",
    "\n",
    "- If the gain is smaller than γ, we would better not add that branch. This is **Pruning**  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
